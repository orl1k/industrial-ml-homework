{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "_Это задание очень похоже на задание, которое вы уже делали на Hadoop, так и задумано, оцените, насколько проще оно решается на Spark._\n",
    "\n",
    "Будем использовать логи прослушивания музыкальных исполнителей в сервисе Яндекс.Музыка.\n",
    "\n",
    "Файл `events.csv` содержит записи вида `Пользователь,Исполнитель,Число прослушиваний,Число пропусков`:\n",
    "```csv\n",
    "userId,artistId,plays,skips\n",
    "0,335,1,0\n",
    "0,708,1,0\n",
    "0,710,2,1\n",
    "0,815,1,1\n",
    "```\n",
    "\n",
    "Вам необходимо проделать следующее:\n",
    "1. **Оставьте в данных только тех пользователей, для которых сумма plays строго больше 2000. Сколько таких пользователей?**\n",
    "2. **В отфильтрованных на первом шаге данных найдите 5 самых популярных по числу пользователей исполнителей (идентификаторы).**\n",
    "\n",
    "Детали:\n",
    "1. Давайте считать, что список прослушиваний одного пользователя всегда помещается в память.\n",
    "\n",
    "Решение сохраните в файл `result.json`. Пример содержимого файла:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 123,\n",
    "    \"q2\": [\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,artistId,plays,skips\r\n",
      "0,335,1,0\r\n",
      "0,708,1,0\r\n",
      "0,710,2,1\r\n",
      "0,815,1,1\r\n"
     ]
    }
   ],
   "source": [
    "# пример содержимого файла\n",
    "! head -n 5 yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 jovyan supergroup        254 2021-05-01 22:54 /yandex_music/README.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup      3.7 M 2021-05-01 22:54 /yandex_music/artists.jsonl\r\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2021-05-01 22:54 /yandex_music/events.csv\r\n"
     ]
    }
   ],
   "source": [
    "# копируем файлы в HDFS\n",
    "! hadoop fs -copyFromLocal yandex_music /\n",
    "! hadoop fs -ls -h /yandex_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "# http://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>artistId</th>\n",
       "      <th>plays</th>\n",
       "      <th>skips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>710</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userId artistId plays skips\n",
       "0      0      335     1     0\n",
       "1      0      708     1     0\n",
       "2      0      710     2     1\n",
       "3      0      815     1     1\n",
       "4      0      880     1     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем csv как Spark DataFrame\n",
    "events = se.read.csv(\"hdfs:///yandex_music/events.csv\", header=True)  # в первой строке у нас заголовок\n",
    "events.registerTempTable(\"events\")\n",
    "events.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='0', artistId='335', plays='1', skips='0'),\n",
       " Row(userId='0', artistId='708', plays='1', skips='0'),\n",
       " Row(userId='0', artistId='710', plays='2', skips='1'),\n",
       " Row(userId='0', artistId='815', plays='1', skips='1'),\n",
       " Row(userId='0', artistId='880', plays='1', skips='1')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# можно сконвертировать этот DataFrame в RDD\n",
    "events.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|answer1|\n",
      "+-------+\n",
      "|   1705|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.sql('''\n",
    "select count(*) as answer1 from\n",
    "(\n",
    "select userId, sum(plays) as plays_sum \n",
    "from events \n",
    "group by userId\n",
    ") t\n",
    "where t.plays_sum > 2000\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|artistId|plays_sum|\n",
      "+--------+---------+\n",
      "|   11368|     1421|\n",
      "|    3629|     1274|\n",
      "|     259|     1221|\n",
      "|   44148|     1191|\n",
      "|   23524|     1167|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.sql('''\n",
    "select t2.artistId, count(t2.userId) plays_count from\n",
    "(\n",
    "select userId from\n",
    "(\n",
    "select userId, sum(plays) as plays_sum \n",
    "from events\n",
    "group by userId\n",
    ") t\n",
    "where t.plays_sum > 2000\n",
    ") t1\n",
    "join events t2\n",
    "on t1.userId = t2.userId\n",
    "group by artistId\n",
    "order by plays_count desc\n",
    "limit 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting result.json\n"
     ]
    }
   ],
   "source": [
    "%%file result.json\n",
    "{\n",
    "    \"q1\": 1705,\n",
    "    \"q2\": [\n",
    "        11368,\n",
    "        3629,\n",
    "        259,\n",
    "        44148,\n",
    "        23524\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# останавливаем Spark (и YARN приложение)\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
