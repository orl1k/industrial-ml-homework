{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Давайте рассмотрим задачу классификации на следующие два класса:\n",
    "- 1 для 'American_movie_actors'\n",
    "- 0 для 'American_stage_actors'\n",
    "\n",
    "На лекциях мы обсуждали, что вместо словаря можно использовать хэширование.\n",
    "\n",
    "Вам предлагается проверить, как поведет себя модель после использования хэширования и ответить на следующие вопросы:\n",
    "1. **Какой roc_auc_score на тестовой выборке получается при использовании словаря?**\n",
    "2. **Какой roc_auc_score на тестовой выборке получается при переходе со словаря на хэширование?**\n",
    "\n",
    "Детали:\n",
    "1. Разбейте выборки на обучающую и тестовую по четности `id` статьи: четные в обучение, нечетные в тест. Только по тренировочной части мы считаем градиенты!\n",
    "2. Для подсчета roc_auc_score вам нужно получить предсказания и истинные ответы для примеров из тестовой выборки. Все пары (предсказание, ответ) помещаются в память, воспользуйтесь этим!\n",
    "3. В качестве хэш-функции используйте `murmurhash3_32(x) % 2**20`.\n",
    "4. Зафиксируйте random seed в начальном приближении весов: `np.random.seed(0); weights = np.random.random(...)`\n",
    "5. Обучите 500 эпох с шагом 0.3. После каждой эпохи вызывайте `weights_broadcast.destroy()` для удаления broadcast переменной, чтобы не закончилась память. \n",
    "6. Вот так выглядит roc_auc_score на тестовой выборке от числа эпох (чем больше roc_auc_score, тем лучше):\n",
    "<img src=\"images/test_auc.png\" width=\"600px\"></img>\n",
    "\n",
    "Решение сохраните в файл `result.json`. Пример содержимого файла:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 0.123,\n",
    "    \"q2\": 0.456\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/wiki/wiki.jsonl': File exists\r\n",
      "copyFromLocal: `/wiki/README.txt': File exists\r\n",
      "copyFromLocal: `/wiki/categories.jsonl': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal wiki /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "\n",
       "                                         url  \n",
       "0  https://simple.wikipedia.org/wiki?curid=1  \n",
       "1  https://simple.wikipedia.org/wiki?curid=2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = se.read.json(\"hdfs:///wiki/wiki.jsonl\")\n",
    "wiki.registerTempTable(\"wiki\")\n",
    "wiki.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  page_id\n",
       "0   Months        1\n",
       "1   Months        2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = se.read.json(\"hdfs:///wiki/categories.jsonl\")\n",
    "categories.registerTempTable(\"categories\")\n",
    "categories.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50 Cent\\n\\n50 Cent (also known as Fitty\" or \"F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anne Ramsey\\n\\nAnne Ramsey (March 27, 1929 – A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  50 Cent\\n\\n50 Cent (also known as Fitty\" or \"F...       1\n",
       "1  Anne Ramsey\\n\\nAnne Ramsey (March 27, 1929 – A...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_joined = se.sql(\"\"\"\n",
    "select\n",
    "    wiki.text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'American_stage_actors')\n",
    "and wiki.id % 2 = 0\n",
    "\"\"\")\n",
    "train_joined.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danny Jacobs (actor)\\n\\nDaniel Charles Jacobs,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conrad Bain\\n\\nConrad Stafford Bain (February ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Danny Jacobs (actor)\\n\\nDaniel Charles Jacobs,...       1\n",
       "1  Conrad Bain\\n\\nConrad Stafford Bain (February ...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_joined = se.sql(\"\"\"\n",
    "select\n",
    "    wiki.text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'American_stage_actors')\n",
    "and wiki.id % 2 = 1\n",
    "\"\"\")\n",
    "test_joined.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # непечатные символы заменяем на пробел\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # и пунктуацию\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.7 ms, sys: 28.4 ms, total: 111 ms\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# индексы нужны для векторизации текстов\n",
    "word_to_index = {word: index for index, (word, count) in enumerate(top_word_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вторая опция: broadcast переменная\n",
    "word_to_index_broadcast = sc.broadcast(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.tokenize(text)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.udf.register(\"tokenize\", tokenize, \"array<string>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+--------------------+\n",
      "| id|                text| title|                 url|\n",
      "+---+--------------------+------+--------------------+\n",
      "|  1|April\n",
      "\n",
      "April is t...| April|https://simple.wi...|\n",
      "|  2|August\n",
      "\n",
      "August (A...|August|https://simple.wi...|\n",
      "|  6|Art\n",
      "\n",
      "Art is a cre...|   Art|https://simple.wi...|\n",
      "+---+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.sql('''select * from wiki limit 3''').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5599"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_joined.rdd.map(mapper)\n",
    "test = test_joined.rdd.map(mapper)\n",
    "train.cache()  # кэшируем датасет в RAM\n",
    "train.count() + test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  531,  7751,    20,    33,    11,    25,  3244,     4,    13,\n",
       "            35,  4850,   733,  2384,  2618,  2669,   287,  4920,     5,\n",
       "          1190,   737,    18,     6,    22,     1,  4114,    53,   225,\n",
       "            62,     9,   115,   164,   778,    28,   388,    78,    42,\n",
       "          1171,  6981,  1766,  1438,    75,   517,  2089,   338,   481,\n",
       "          3066,    51,   313,   194,    72,   187,   802,   413,     2,\n",
       "          3177,    45, 45629,  1739,   324,     7,   365,  1677,   208,\n",
       "           397,  1272,   886,   226,   106,  2135,  9907,    30,    73,\n",
       "           704,   236,   445,   648,    12,  1928,  4292,  5057,   201,\n",
       "           303,     0,   129,   271,    48,   190,   954,  1321,   100,\n",
       "            94,    23,   613,  2840,     3,   712,   381,   270,  1093,\n",
       "          1275,  1468,   317,  2899,  3988,     8,   182,    27,   440,\n",
       "           468,  1493,   793, 23164,   188,  1530,   150,   459,   133,\n",
       "           134,   335,   963,  1805,    10,  1169,  9656,   102,   523,\n",
       "           344,    54,   990,  6772,    17,   444,   554,  1357,   158,\n",
       "            39,    24,   263,    40,    63, 27991,   584,    50,  8216,\n",
       "            87,  1822,  8855,   430,  1240,  6790,  3149,  1906,  1908,\n",
       "          2756,  3466,    82, 12853,    15, 19614,  1371, 15726,  9518,\n",
       "          1108,  7993,    44,  2377,   320,  6455,  3334,   337,   998,\n",
       "          2833,  1408,  4277,   372,   922, 20422, 21301,   111,    14,\n",
       "           615, 16512,    26, 12325, 21541,  4082,   688,    95,  1952,\n",
       "           593,    21,   846,   166,   107,    77,   595,  1436,   961,\n",
       "          1718,    60,   856,   264,   475,    69,  4035,   296,   241,\n",
       "          5856,   178,    55,  1217,  2423,  2556,  1686,  2019, 33000,\n",
       "          1049,   380]),\n",
       "  array([ 0.03465347,  0.03217822,  0.00990099,  0.00247525,  0.0049505 ,\n",
       "          0.00742574,  0.00247525,  0.00990099,  0.0049505 ,  0.0049505 ,\n",
       "          0.0049505 ,  0.00742574,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.00247525,  0.00247525,  0.02475248,  0.00247525,  0.00247525,\n",
       "          0.02970297,  0.02475248,  0.00247525,  0.02227723,  0.00247525,\n",
       "          0.00742574,  0.00247525,  0.00247525,  0.01485149,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.02227723,  0.00247525,  0.0049505 ,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.0049505 ,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.02227723,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00742574,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.01237624,  0.00247525,  0.00247525,  0.00990099,  0.00247525,\n",
       "          0.0049505 ,  0.00990099,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.0049505 ,  0.04455446,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00742574,  0.0049505 ,  0.00247525,  0.01485149,\n",
       "          0.00742574,  0.00742574,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00742574,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.01237624,  0.00742574,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.0049505 ,  0.00990099,  0.00742574,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00990099,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525]),\n",
       "  1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1. + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # здесь накапливаем вклад в градиент\n",
    "    gradient = np.zeros(len(weights_broadcast.value))\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # делаем предсказание с текущими весами\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # добавляем в накопитель градиента\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # считаем потери\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество примеров\n",
    "N = train.count() + test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.308791626062\n",
      "epoch: 1 loss: 0.30116744249\n",
      "epoch: 2 loss: 0.29717618833\n",
      "epoch: 3 loss: 0.295568934516\n",
      "epoch: 4 loss: 0.294422814126\n",
      "epoch: 5 loss: 0.29349934133\n",
      "epoch: 6 loss: 0.292700359914\n",
      "epoch: 7 loss: 0.291988544384\n",
      "epoch: 8 loss: 0.291342982542\n",
      "epoch: 9 loss: 0.290749802393\n",
      "epoch: 10 loss: 0.290199061212\n",
      "epoch: 11 loss: 0.289683342704\n",
      "epoch: 12 loss: 0.289197023653\n",
      "epoch: 13 loss: 0.288735773888\n",
      "epoch: 14 loss: 0.288296216688\n",
      "epoch: 15 loss: 0.287875682339\n",
      "epoch: 16 loss: 0.287472031016\n",
      "epoch: 17 loss: 0.287083523065\n",
      "epoch: 18 loss: 0.286708724038\n",
      "epoch: 19 loss: 0.28634643452\n",
      "epoch: 20 loss: 0.285995638069\n",
      "epoch: 21 loss: 0.285655462309\n",
      "epoch: 22 loss: 0.285325149664\n",
      "epoch: 23 loss: 0.285004035174\n",
      "epoch: 24 loss: 0.284691529553\n",
      "epoch: 25 loss: 0.284387106123\n",
      "epoch: 26 loss: 0.284090290648\n",
      "epoch: 27 loss: 0.283800653351\n",
      "epoch: 28 loss: 0.283517802557\n",
      "epoch: 29 loss: 0.283241379602\n",
      "epoch: 30 loss: 0.282971054681\n",
      "epoch: 31 loss: 0.282706523444\n",
      "epoch: 32 loss: 0.282447504164\n",
      "epoch: 33 loss: 0.282193735353\n",
      "epoch: 34 loss: 0.281944973735\n",
      "epoch: 35 loss: 0.281700992513\n",
      "epoch: 36 loss: 0.281461579862\n",
      "epoch: 37 loss: 0.281226537613\n",
      "epoch: 38 loss: 0.280995680095\n",
      "epoch: 39 loss: 0.280768833112\n",
      "epoch: 40 loss: 0.280545833025\n",
      "epoch: 41 loss: 0.280326525936\n",
      "epoch: 42 loss: 0.280110766945\n",
      "epoch: 43 loss: 0.279898419486\n",
      "epoch: 44 loss: 0.279689354724\n",
      "epoch: 45 loss: 0.279483450996\n",
      "epoch: 46 loss: 0.279280593317\n",
      "epoch: 47 loss: 0.279080672914\n",
      "epoch: 48 loss: 0.278883586808\n",
      "epoch: 49 loss: 0.278689237421\n",
      "epoch: 50 loss: 0.278497532229\n",
      "epoch: 51 loss: 0.278308383426\n",
      "epoch: 52 loss: 0.278121707622\n",
      "epoch: 53 loss: 0.277937425566\n",
      "epoch: 54 loss: 0.277755461886\n",
      "epoch: 55 loss: 0.277575744849\n",
      "epoch: 56 loss: 0.277398206136\n",
      "epoch: 57 loss: 0.277222780641\n",
      "epoch: 58 loss: 0.277049406275\n",
      "epoch: 59 loss: 0.276878023791\n",
      "epoch: 60 loss: 0.276708576614\n",
      "epoch: 61 loss: 0.276541010694\n",
      "epoch: 62 loss: 0.276375274352\n",
      "epoch: 63 loss: 0.276211318155\n",
      "epoch: 64 loss: 0.276049094787\n",
      "epoch: 65 loss: 0.275888558932\n",
      "epoch: 66 loss: 0.275729667162\n",
      "epoch: 67 loss: 0.275572377842\n",
      "epoch: 68 loss: 0.275416651028\n",
      "epoch: 69 loss: 0.275262448377\n",
      "epoch: 70 loss: 0.275109733069\n",
      "epoch: 71 loss: 0.274958469721\n",
      "epoch: 72 loss: 0.274808624318\n",
      "epoch: 73 loss: 0.274660164141\n",
      "epoch: 74 loss: 0.274513057703\n",
      "epoch: 75 loss: 0.274367274688\n",
      "epoch: 76 loss: 0.274222785892\n",
      "epoch: 77 loss: 0.274079563167\n",
      "epoch: 78 loss: 0.273937579376\n",
      "epoch: 79 loss: 0.273796808338\n",
      "epoch: 80 loss: 0.273657224787\n",
      "epoch: 81 loss: 0.273518804325\n",
      "epoch: 82 loss: 0.273381523388\n",
      "epoch: 83 loss: 0.273245359202\n",
      "epoch: 84 loss: 0.273110289747\n",
      "epoch: 85 loss: 0.272976293728\n",
      "epoch: 86 loss: 0.272843350539\n",
      "epoch: 87 loss: 0.272711440231\n",
      "epoch: 88 loss: 0.272580543487\n",
      "epoch: 89 loss: 0.272450641593\n",
      "epoch: 90 loss: 0.272321716412\n",
      "epoch: 91 loss: 0.272193750357\n",
      "epoch: 92 loss: 0.272066726375\n",
      "epoch: 93 loss: 0.271940627916\n",
      "epoch: 94 loss: 0.271815438918\n",
      "epoch: 95 loss: 0.271691143785\n",
      "epoch: 96 loss: 0.271567727369\n",
      "epoch: 97 loss: 0.27144517495\n",
      "epoch: 98 loss: 0.271323472224\n",
      "epoch: 99 loss: 0.271202605279\n",
      "epoch: 100 loss: 0.271082560587\n",
      "epoch: 101 loss: 0.270963324984\n",
      "epoch: 102 loss: 0.27084488566\n",
      "epoch: 103 loss: 0.270727230142\n",
      "epoch: 104 loss: 0.270610346285\n",
      "epoch: 105 loss: 0.270494222258\n",
      "epoch: 106 loss: 0.270378846531\n",
      "epoch: 107 loss: 0.270264207867\n",
      "epoch: 108 loss: 0.270150295309\n",
      "epoch: 109 loss: 0.270037098172\n",
      "epoch: 110 loss: 0.269924606031\n",
      "epoch: 111 loss: 0.269812808716\n",
      "epoch: 112 loss: 0.269701696298\n",
      "epoch: 113 loss: 0.269591259085\n",
      "epoch: 114 loss: 0.269481487611\n",
      "epoch: 115 loss: 0.269372372629\n",
      "epoch: 116 loss: 0.269263905107\n",
      "epoch: 117 loss: 0.269156076216\n",
      "epoch: 118 loss: 0.269048877326\n",
      "epoch: 119 loss: 0.268942299999\n",
      "epoch: 120 loss: 0.268836335984\n",
      "epoch: 121 loss: 0.26873097721\n",
      "epoch: 122 loss: 0.268626215778\n",
      "epoch: 123 loss: 0.268522043961\n",
      "epoch: 124 loss: 0.268418454194\n",
      "epoch: 125 loss: 0.268315439072\n",
      "epoch: 126 loss: 0.268212991343\n",
      "epoch: 127 loss: 0.268111103905\n",
      "epoch: 128 loss: 0.2680097698\n",
      "epoch: 129 loss: 0.267908982212\n",
      "epoch: 130 loss: 0.267808734459\n",
      "epoch: 131 loss: 0.267709019995\n",
      "epoch: 132 loss: 0.267609832401\n",
      "epoch: 133 loss: 0.267511165383\n",
      "epoch: 134 loss: 0.267413012769\n",
      "epoch: 135 loss: 0.267315368505\n",
      "epoch: 136 loss: 0.267218226652\n",
      "epoch: 137 loss: 0.267121581384\n",
      "epoch: 138 loss: 0.267025426982\n",
      "epoch: 139 loss: 0.266929757835\n",
      "epoch: 140 loss: 0.266834568432\n",
      "epoch: 141 loss: 0.266739853366\n",
      "epoch: 142 loss: 0.266645607325\n",
      "epoch: 143 loss: 0.266551825094\n",
      "epoch: 144 loss: 0.266458501549\n",
      "epoch: 145 loss: 0.266365631658\n",
      "epoch: 146 loss: 0.266273210476\n",
      "epoch: 147 loss: 0.266181233145\n",
      "epoch: 148 loss: 0.26608969489\n",
      "epoch: 149 loss: 0.265998591018\n",
      "epoch: 150 loss: 0.265907916916\n",
      "epoch: 151 loss: 0.265817668048\n",
      "epoch: 152 loss: 0.265727839955\n",
      "epoch: 153 loss: 0.265638428251\n",
      "epoch: 154 loss: 0.265549428624\n",
      "epoch: 155 loss: 0.26546083683\n",
      "epoch: 156 loss: 0.265372648698\n",
      "epoch: 157 loss: 0.26528486012\n",
      "epoch: 158 loss: 0.265197467058\n",
      "epoch: 159 loss: 0.265110465535\n",
      "epoch: 160 loss: 0.26502385164\n",
      "epoch: 161 loss: 0.264937621522\n",
      "epoch: 162 loss: 0.264851771389\n",
      "epoch: 163 loss: 0.264766297511\n",
      "epoch: 164 loss: 0.264681196213\n",
      "epoch: 165 loss: 0.264596463877\n",
      "epoch: 166 loss: 0.264512096941\n",
      "epoch: 167 loss: 0.264428091897\n",
      "epoch: 168 loss: 0.264344445288\n",
      "epoch: 169 loss: 0.264261153711\n",
      "epoch: 170 loss: 0.264178213811\n",
      "epoch: 171 loss: 0.264095622287\n",
      "epoch: 172 loss: 0.264013375881\n",
      "epoch: 173 loss: 0.263931471387\n",
      "epoch: 174 loss: 0.263849905644\n",
      "epoch: 175 loss: 0.263768675536\n",
      "epoch: 176 loss: 0.263687777993\n",
      "epoch: 177 loss: 0.263607209988\n",
      "epoch: 178 loss: 0.263526968538\n",
      "epoch: 179 loss: 0.263447050701\n",
      "epoch: 180 loss: 0.263367453578\n",
      "epoch: 181 loss: 0.263288174308\n",
      "epoch: 182 loss: 0.263209210072\n",
      "epoch: 183 loss: 0.26313055809\n",
      "epoch: 184 loss: 0.263052215618\n",
      "epoch: 185 loss: 0.262974179953\n",
      "epoch: 186 loss: 0.262896448425\n",
      "epoch: 187 loss: 0.262819018404\n",
      "epoch: 188 loss: 0.262741887292\n",
      "epoch: 189 loss: 0.262665052528\n",
      "epoch: 190 loss: 0.262588511586\n",
      "epoch: 191 loss: 0.26251226197\n",
      "epoch: 192 loss: 0.262436301221\n",
      "epoch: 193 loss: 0.26236062691\n",
      "epoch: 194 loss: 0.26228523664\n",
      "epoch: 195 loss: 0.262210128047\n",
      "epoch: 196 loss: 0.262135298794\n",
      "epoch: 197 loss: 0.262060746579\n",
      "epoch: 198 loss: 0.261986469126\n",
      "epoch: 199 loss: 0.261912464189\n",
      "epoch: 200 loss: 0.261838729551\n",
      "epoch: 201 loss: 0.261765263023\n",
      "epoch: 202 loss: 0.261692062444\n",
      "epoch: 203 loss: 0.261619125679\n",
      "epoch: 204 loss: 0.26154645062\n",
      "epoch: 205 loss: 0.261474035187\n",
      "epoch: 206 loss: 0.261401877324\n",
      "epoch: 207 loss: 0.261329975001\n",
      "epoch: 208 loss: 0.261258326211\n",
      "epoch: 209 loss: 0.261186928976\n",
      "epoch: 210 loss: 0.261115781338\n",
      "epoch: 211 loss: 0.261044881365\n",
      "epoch: 212 loss: 0.260974227146\n",
      "epoch: 213 loss: 0.260903816797\n",
      "epoch: 214 loss: 0.260833648452\n",
      "epoch: 215 loss: 0.260763720271\n",
      "epoch: 216 loss: 0.260694030433\n",
      "epoch: 217 loss: 0.260624577142\n",
      "epoch: 218 loss: 0.260555358618\n",
      "epoch: 219 loss: 0.260486373108\n",
      "epoch: 220 loss: 0.260417618874\n",
      "epoch: 221 loss: 0.260349094203\n",
      "epoch: 222 loss: 0.260280797398\n",
      "epoch: 223 loss: 0.260212726784\n",
      "epoch: 224 loss: 0.260144880704\n",
      "epoch: 225 loss: 0.260077257521\n",
      "epoch: 226 loss: 0.260009855616\n",
      "epoch: 227 loss: 0.259942673388\n",
      "epoch: 228 loss: 0.259875709257\n",
      "epoch: 229 loss: 0.259808961656\n",
      "epoch: 230 loss: 0.25974242904\n",
      "epoch: 231 loss: 0.259676109879\n",
      "epoch: 232 loss: 0.259610002662\n",
      "epoch: 233 loss: 0.259544105891\n",
      "epoch: 234 loss: 0.259478418089\n",
      "epoch: 235 loss: 0.259412937793\n",
      "epoch: 236 loss: 0.259347663557\n",
      "epoch: 237 loss: 0.25928259395\n",
      "epoch: 238 loss: 0.259217727557\n",
      "epoch: 239 loss: 0.259153062978\n",
      "epoch: 240 loss: 0.25908859883\n",
      "epoch: 241 loss: 0.259024333742\n",
      "epoch: 242 loss: 0.25896026636\n",
      "epoch: 243 loss: 0.258896395344\n",
      "epoch: 244 loss: 0.258832719368\n",
      "epoch: 245 loss: 0.258769237121\n",
      "epoch: 246 loss: 0.258705947304\n",
      "epoch: 247 loss: 0.258642848634\n",
      "epoch: 248 loss: 0.25857993984\n",
      "epoch: 249 loss: 0.258517219665\n",
      "epoch: 250 loss: 0.258454686865\n",
      "epoch: 251 loss: 0.258392340209\n",
      "epoch: 252 loss: 0.258330178479\n",
      "epoch: 253 loss: 0.258268200469\n",
      "epoch: 254 loss: 0.258206404986\n",
      "epoch: 255 loss: 0.25814479085\n",
      "epoch: 256 loss: 0.25808335689\n",
      "epoch: 257 loss: 0.258022101952\n",
      "epoch: 258 loss: 0.257961024889\n",
      "epoch: 259 loss: 0.257900124568\n",
      "epoch: 260 loss: 0.257839399867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 261 loss: 0.257778849676\n",
      "epoch: 262 loss: 0.257718472895\n",
      "epoch: 263 loss: 0.257658268436\n",
      "epoch: 264 loss: 0.257598235221\n",
      "epoch: 265 loss: 0.257538372183\n",
      "epoch: 266 loss: 0.257478678265\n",
      "epoch: 267 loss: 0.257419152423\n",
      "epoch: 268 loss: 0.257359793621\n",
      "epoch: 269 loss: 0.257300600832\n",
      "epoch: 270 loss: 0.257241573043\n",
      "epoch: 271 loss: 0.257182709246\n",
      "epoch: 272 loss: 0.257124008447\n",
      "epoch: 273 loss: 0.25706546966\n",
      "epoch: 274 loss: 0.257007091908\n",
      "epoch: 275 loss: 0.256948874224\n",
      "epoch: 276 loss: 0.25689081565\n",
      "epoch: 277 loss: 0.256832915237\n",
      "epoch: 278 loss: 0.256775172047\n",
      "epoch: 279 loss: 0.256717585148\n",
      "epoch: 280 loss: 0.256660153618\n",
      "epoch: 281 loss: 0.256602876544\n",
      "epoch: 282 loss: 0.256545753022\n",
      "epoch: 283 loss: 0.256488782155\n",
      "epoch: 284 loss: 0.256431963057\n",
      "epoch: 285 loss: 0.256375294847\n",
      "epoch: 286 loss: 0.256318776654\n",
      "epoch: 287 loss: 0.256262407615\n",
      "epoch: 288 loss: 0.256206186875\n",
      "epoch: 289 loss: 0.256150113587\n",
      "epoch: 290 loss: 0.256094186911\n",
      "epoch: 291 loss: 0.256038406015\n",
      "epoch: 292 loss: 0.255982770074\n",
      "epoch: 293 loss: 0.255927278274\n",
      "epoch: 294 loss: 0.255871929802\n",
      "epoch: 295 loss: 0.255816723859\n",
      "epoch: 296 loss: 0.255761659649\n",
      "epoch: 297 loss: 0.255706736384\n",
      "epoch: 298 loss: 0.255651953283\n",
      "epoch: 299 loss: 0.255597309573\n",
      "epoch: 300 loss: 0.255542804488\n",
      "epoch: 301 loss: 0.255488437267\n",
      "epoch: 302 loss: 0.255434207157\n",
      "epoch: 303 loss: 0.255380113411\n",
      "epoch: 304 loss: 0.255326155289\n",
      "epoch: 305 loss: 0.255272332058\n",
      "epoch: 306 loss: 0.255218642991\n",
      "epoch: 307 loss: 0.255165087367\n",
      "epoch: 308 loss: 0.25511166447\n",
      "epoch: 309 loss: 0.255058373594\n",
      "epoch: 310 loss: 0.255005214035\n",
      "epoch: 311 loss: 0.254952185098\n",
      "epoch: 312 loss: 0.254899286092\n",
      "epoch: 313 loss: 0.254846516333\n",
      "epoch: 314 loss: 0.254793875142\n",
      "epoch: 315 loss: 0.254741361846\n",
      "epoch: 316 loss: 0.254688975779\n",
      "epoch: 317 loss: 0.25463671628\n",
      "epoch: 318 loss: 0.254584582692\n",
      "epoch: 319 loss: 0.254532574365\n",
      "epoch: 320 loss: 0.254480690654\n",
      "epoch: 321 loss: 0.25442893092\n",
      "epoch: 322 loss: 0.254377294529\n",
      "epoch: 323 loss: 0.254325780851\n",
      "epoch: 324 loss: 0.254274389264\n",
      "epoch: 325 loss: 0.254223119149\n",
      "epoch: 326 loss: 0.254171969891\n",
      "epoch: 327 loss: 0.254120940884\n",
      "epoch: 328 loss: 0.254070031523\n",
      "epoch: 329 loss: 0.254019241209\n",
      "epoch: 330 loss: 0.25396856935\n",
      "epoch: 331 loss: 0.253918015357\n",
      "epoch: 332 loss: 0.253867578645\n",
      "epoch: 333 loss: 0.253817258635\n",
      "epoch: 334 loss: 0.253767054753\n",
      "epoch: 335 loss: 0.253716966428\n",
      "epoch: 336 loss: 0.253666993095\n",
      "epoch: 337 loss: 0.253617134193\n",
      "epoch: 338 loss: 0.253567389165\n",
      "epoch: 339 loss: 0.25351775746\n",
      "epoch: 340 loss: 0.253468238528\n",
      "epoch: 341 loss: 0.253418831828\n",
      "epoch: 342 loss: 0.25336953682\n",
      "epoch: 343 loss: 0.253320352969\n",
      "epoch: 344 loss: 0.253271279743\n",
      "epoch: 345 loss: 0.253222316617\n",
      "epoch: 346 loss: 0.253173463068\n",
      "epoch: 347 loss: 0.253124718578\n",
      "epoch: 348 loss: 0.253076082631\n",
      "epoch: 349 loss: 0.253027554717\n",
      "epoch: 350 loss: 0.252979134331\n",
      "epoch: 351 loss: 0.252930820968\n",
      "epoch: 352 loss: 0.25288261413\n",
      "epoch: 353 loss: 0.252834513323\n",
      "epoch: 354 loss: 0.252786518054\n",
      "epoch: 355 loss: 0.252738627836\n",
      "epoch: 356 loss: 0.252690842185\n",
      "epoch: 357 loss: 0.252643160621\n",
      "epoch: 358 loss: 0.252595582667\n",
      "epoch: 359 loss: 0.25254810785\n",
      "epoch: 360 loss: 0.2525007357\n",
      "epoch: 361 loss: 0.252453465752\n",
      "epoch: 362 loss: 0.252406297542\n",
      "epoch: 363 loss: 0.252359230612\n",
      "epoch: 364 loss: 0.252312264506\n",
      "epoch: 365 loss: 0.25226539877\n",
      "epoch: 366 loss: 0.252218632957\n",
      "epoch: 367 loss: 0.25217196662\n",
      "epoch: 368 loss: 0.252125399317\n",
      "epoch: 369 loss: 0.252078930608\n",
      "epoch: 370 loss: 0.252032560057\n",
      "epoch: 371 loss: 0.251986287231\n",
      "epoch: 372 loss: 0.2519401117\n",
      "epoch: 373 loss: 0.251894033038\n",
      "epoch: 374 loss: 0.25184805082\n",
      "epoch: 375 loss: 0.251802164626\n",
      "epoch: 376 loss: 0.251756374039\n",
      "epoch: 377 loss: 0.251710678643\n",
      "epoch: 378 loss: 0.251665078027\n",
      "epoch: 379 loss: 0.251619571782\n",
      "epoch: 380 loss: 0.251574159502\n",
      "epoch: 381 loss: 0.251528840784\n",
      "epoch: 382 loss: 0.251483615228\n",
      "epoch: 383 loss: 0.251438482435\n",
      "epoch: 384 loss: 0.251393442013\n",
      "epoch: 385 loss: 0.251348493567\n",
      "epoch: 386 loss: 0.251303636711\n",
      "epoch: 387 loss: 0.251258871056\n",
      "epoch: 388 loss: 0.251214196219\n",
      "epoch: 389 loss: 0.251169611819\n",
      "epoch: 390 loss: 0.251125117477\n",
      "epoch: 391 loss: 0.251080712818\n",
      "epoch: 392 loss: 0.251036397467\n",
      "epoch: 393 loss: 0.250992171055\n",
      "epoch: 394 loss: 0.250948033212\n",
      "epoch: 395 loss: 0.250903983574\n",
      "epoch: 396 loss: 0.250860021776\n",
      "epoch: 397 loss: 0.250816147457\n",
      "epoch: 398 loss: 0.25077236026\n",
      "epoch: 399 loss: 0.250728659828\n",
      "epoch: 400 loss: 0.250685045808\n",
      "epoch: 401 loss: 0.250641517848\n",
      "epoch: 402 loss: 0.250598075599\n",
      "epoch: 403 loss: 0.250554718715\n",
      "epoch: 404 loss: 0.250511446852\n",
      "epoch: 405 loss: 0.250468259666\n",
      "epoch: 406 loss: 0.250425156819\n",
      "epoch: 407 loss: 0.250382137973\n",
      "epoch: 408 loss: 0.250339202792\n",
      "epoch: 409 loss: 0.250296350943\n",
      "epoch: 410 loss: 0.250253582096\n",
      "epoch: 411 loss: 0.250210895921\n",
      "epoch: 412 loss: 0.250168292093\n",
      "epoch: 413 loss: 0.250125770285\n",
      "epoch: 414 loss: 0.250083330176\n",
      "epoch: 415 loss: 0.250040971445\n",
      "epoch: 416 loss: 0.249998693774\n",
      "epoch: 417 loss: 0.249956496847\n",
      "epoch: 418 loss: 0.249914380349\n",
      "epoch: 419 loss: 0.249872343969\n",
      "epoch: 420 loss: 0.249830387395\n",
      "epoch: 421 loss: 0.249788510319\n",
      "epoch: 422 loss: 0.249746712435\n",
      "epoch: 423 loss: 0.249704993439\n",
      "epoch: 424 loss: 0.249663353029\n",
      "epoch: 425 loss: 0.249621790902\n",
      "epoch: 426 loss: 0.249580306761\n",
      "epoch: 427 loss: 0.249538900309\n",
      "epoch: 428 loss: 0.249497571251\n",
      "epoch: 429 loss: 0.249456319294\n",
      "epoch: 430 loss: 0.249415144146\n",
      "epoch: 431 loss: 0.249374045517\n",
      "epoch: 432 loss: 0.249333023121\n",
      "epoch: 433 loss: 0.24929207667\n",
      "epoch: 434 loss: 0.249251205881\n",
      "epoch: 435 loss: 0.249210410471\n",
      "epoch: 436 loss: 0.249169690159\n",
      "epoch: 437 loss: 0.249129044667\n",
      "epoch: 438 loss: 0.249088473716\n",
      "epoch: 439 loss: 0.249047977032\n",
      "epoch: 440 loss: 0.24900755434\n",
      "epoch: 441 loss: 0.248967205367\n",
      "epoch: 442 loss: 0.248926929844\n",
      "epoch: 443 loss: 0.248886727501\n",
      "epoch: 444 loss: 0.24884659807\n",
      "epoch: 445 loss: 0.248806541286\n",
      "epoch: 446 loss: 0.248766556884\n",
      "epoch: 447 loss: 0.248726644602\n",
      "epoch: 448 loss: 0.248686804178\n",
      "epoch: 449 loss: 0.248647035353\n",
      "epoch: 450 loss: 0.248607337868\n",
      "epoch: 451 loss: 0.248567711468\n",
      "epoch: 452 loss: 0.248528155896\n",
      "epoch: 453 loss: 0.2484886709\n",
      "epoch: 454 loss: 0.248449256227\n",
      "epoch: 455 loss: 0.248409911627\n",
      "epoch: 456 loss: 0.248370636851\n",
      "epoch: 457 loss: 0.24833143165\n",
      "epoch: 458 loss: 0.248292295779\n",
      "epoch: 459 loss: 0.248253228993\n",
      "epoch: 460 loss: 0.248214231048\n",
      "epoch: 461 loss: 0.248175301702\n",
      "epoch: 462 loss: 0.248136440715\n",
      "epoch: 463 loss: 0.248097647848\n",
      "epoch: 464 loss: 0.248058922861\n",
      "epoch: 465 loss: 0.24802026552\n",
      "epoch: 466 loss: 0.247981675588\n",
      "epoch: 467 loss: 0.247943152832\n",
      "epoch: 468 loss: 0.247904697019\n",
      "epoch: 469 loss: 0.247866307918\n",
      "epoch: 470 loss: 0.247827985299\n",
      "epoch: 471 loss: 0.247789728934\n",
      "epoch: 472 loss: 0.247751538595\n",
      "epoch: 473 loss: 0.247713414056\n",
      "epoch: 474 loss: 0.247675355092\n",
      "epoch: 475 loss: 0.247637361479\n",
      "epoch: 476 loss: 0.247599432996\n",
      "epoch: 477 loss: 0.247561569421\n",
      "epoch: 478 loss: 0.247523770534\n",
      "epoch: 479 loss: 0.247486036117\n",
      "epoch: 480 loss: 0.247448365952\n",
      "epoch: 481 loss: 0.247410759822\n",
      "epoch: 482 loss: 0.247373217514\n",
      "epoch: 483 loss: 0.247335738812\n",
      "epoch: 484 loss: 0.247298323505\n",
      "epoch: 485 loss: 0.24726097138\n",
      "epoch: 486 loss: 0.247223682227\n",
      "epoch: 487 loss: 0.247186455837\n",
      "epoch: 488 loss: 0.247149292002\n",
      "epoch: 489 loss: 0.247112190515\n",
      "epoch: 490 loss: 0.247075151169\n",
      "epoch: 491 loss: 0.24703817376\n",
      "epoch: 492 loss: 0.247001258084\n",
      "epoch: 493 loss: 0.246964403939\n",
      "epoch: 494 loss: 0.246927611122\n",
      "epoch: 495 loss: 0.246890879434\n",
      "epoch: 496 loss: 0.246854208676\n",
      "epoch: 497 loss: 0.246817598647\n",
      "epoch: 498 loss: 0.246781049153\n",
      "epoch: 499 loss: 0.246744559995\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "np.random.seed(0)\n",
    "\n",
    "# случайные веса\n",
    "weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# эпохи градиентного спуска\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # считаем градиент\n",
    "    gradient = (\n",
    "        train\n",
    "        .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # обновляем веса\n",
    "    weights -= 0.3 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(weights_broadcast, example):\n",
    "    indices, values, _ = example\n",
    "    p = sigmoid(values.dot(weights_broadcast[indices]))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test.map(lambda x: x[2]).collect()\n",
    "y_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75184517807023765,\n",
       " 0.63053430593689674,\n",
       " 0.63053430593689674,\n",
       " 0.82985870394448125,\n",
       " 0.82985870394448125]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = test.map(lambda x: evaluate(weights, x)).collect()\n",
    "y_score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68688612967379636"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# y_true - настоящие классы\n",
    "# y_score - вероятности класса 1\n",
    "# https://ru.wikipedia.org/wiki/ROC-кривая\n",
    "\n",
    "answer1 = roc_auc_score(y_true, y_score)\n",
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.utils.murmurhash.murmurhash3_32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.udf.register(\"murmurhash3_32\", murmurhash3_32, \"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    d = {}\n",
    "    for word in words:\n",
    "        index = murmurhash3_32(word) % (2**20)\n",
    "        d[index] = d.get(index, 0) + 1\n",
    "        \n",
    "    indices = []\n",
    "    values = []\n",
    "    \n",
    "    for i in d:\n",
    "        indices.append(i)\n",
    "        values.append(d[i] / float(len(words)))\n",
    "        \n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5599"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_joined.rdd.map(mapper)\n",
    "test = test_joined.rdd.map(mapper)\n",
    "train.cache()  # кэшируем датасет в RAM\n",
    "train.count() + test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 333032,   12125,  554005,  626138,  494108,  791332,  408423,\n",
       "          403610,  144749,  715111,  510525,  193227, 1033652,  388113,\n",
       "          628081,  885734,  315967,  288863,  868051,  716217,  922115,\n",
       "          764163,  824209, 1013040,  828689,  691061,  681351,  739837,\n",
       "          792415,  735689, 1021892,  687296,  622766,  707072,  790616,\n",
       "         1000432,  372702,  202138,  513188,  766658,  216527,   97469,\n",
       "           42853,  189343,  615763,  779831,  231830,  232512,  632395,\n",
       "          131095,  563656,  992412,   72944,  451990,  354738,   58931,\n",
       "          242292,  466487,  798850,  754507,  174171,  824246,   69756,\n",
       "           61418,  787517,   30408,  939348,  932419,  105848,  647201,\n",
       "           55006,   59416,   25166,  563900,  405050,  900432,  711577,\n",
       "          290475,  293337,  523298,  343796,  322712,  595059,  276572,\n",
       "          761698,  824957,  602552,  892620,  240380,  326804,  799280,\n",
       "            1160,  484299,  402515,  460924,  640596,  879274,  479532,\n",
       "          483370,  863444,  865698,  341586,  338426,  737879,  751832,\n",
       "          351821,  140020,  465608,  329790,   66007,  100044,  823807,\n",
       "          887334,  497004,  505672,  565720,  553858,  510156,  701885,\n",
       "          678600,  262638,  899632,  952913, 1021683,  258134,  858933,\n",
       "          972205,  463172,  217305,  105872,  176900,  305281,  648030,\n",
       "          519706,  190312,  144081,  713750, 1014677,  923174,  868343,\n",
       "          170062,  668440, 1036945,  777676,  623249,  618065,  338809,\n",
       "          464469,   72745,  829667,   23395,  344009,  282677,  107245,\n",
       "          442808,  583809,  544947,  722625,  247327,  513024,  399345,\n",
       "          707313,  360001, 1006559,  949863,  693870,  434169,  730328,\n",
       "          151136,  489209,  803687,  221281,  446200,  634201,  536648,\n",
       "          547244, 1041281,  290960,  619937,  384726,   75648,  493879,\n",
       "          699305,  302201,  716692,  878736,  134226,  873602,  840318,\n",
       "          536292,  357756,  529134,  663205,  329822,  172202,  512176,\n",
       "          738909,  569661,  661528,  507241,  306241,  247787,  586898,\n",
       "         1012959,  359461,  538031, 1041977,  358081,  408827,  570533,\n",
       "          267330,  227230,  242096,  361750,  415230,   88427,  332346,\n",
       "          142929,   58803,  448466,  906644,  736253,  886488]),\n",
       "  array([ 0.03465347,  0.03217822,  0.00990099,  0.00247525,  0.0049505 ,\n",
       "          0.00247525,  0.00742574,  0.00247525,  0.00990099,  0.0049505 ,\n",
       "          0.0049505 ,  0.0049505 ,  0.00742574,  0.00247525,  0.00247525,\n",
       "          0.00990099,  0.00247525,  0.00247525,  0.02475248,  0.00247525,\n",
       "          0.00247525,  0.02970297,  0.02475248,  0.00247525,  0.02227723,\n",
       "          0.00247525,  0.00742574,  0.00247525,  0.00247525,  0.01485149,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.02227723,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.0049505 ,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.02227723,\n",
       "          0.00247525,  0.00247525,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00742574,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.01237624,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.00247525,  0.0049505 ,  0.00990099,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.0049505 ,  0.0049505 ,  0.04455446,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00742574,\n",
       "          0.0049505 ,  0.00247525,  0.01485149,  0.00742574,  0.00742574,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00742574,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.01237624,  0.00742574,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.0049505 ,\n",
       "          0.0049505 ,  0.00990099,  0.00742574,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00990099,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525]),\n",
       "  1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # здесь накапливаем вклад в градиент\n",
    "    gradient = np.zeros(2**20)\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # делаем предсказание с текущими весами\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # добавляем в накопитель градиента\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # считаем потери\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.309970402493\n",
      "epoch: 1 loss: 0.301442760339\n",
      "epoch: 2 loss: 0.296940645776\n",
      "epoch: 3 loss: 0.29527856365\n",
      "epoch: 4 loss: 0.29411565607\n",
      "epoch: 5 loss: 0.293188234502\n",
      "epoch: 6 loss: 0.292387369813\n",
      "epoch: 7 loss: 0.291673929341\n",
      "epoch: 8 loss: 0.291026592678\n",
      "epoch: 9 loss: 0.290431437434\n",
      "epoch: 10 loss: 0.2898785317\n",
      "epoch: 11 loss: 0.289360472421\n",
      "epoch: 12 loss: 0.288871645219\n",
      "epoch: 13 loss: 0.288407724856\n",
      "epoch: 14 loss: 0.287965337758\n",
      "epoch: 15 loss: 0.287541816803\n",
      "epoch: 16 loss: 0.287135024945\n",
      "epoch: 17 loss: 0.286743225815\n",
      "epoch: 18 loss: 0.286364988844\n",
      "epoch: 19 loss: 0.285999119063\n",
      "epoch: 20 loss: 0.285644604954\n",
      "epoch: 21 loss: 0.28530057943\n",
      "epoch: 22 loss: 0.284966290459\n",
      "epoch: 23 loss: 0.28464107878\n",
      "epoch: 24 loss: 0.28432436087\n",
      "epoch: 25 loss: 0.28401561581\n",
      "epoch: 26 loss: 0.283714375059\n",
      "epoch: 27 loss: 0.283420214424\n",
      "epoch: 28 loss: 0.283132747674\n",
      "epoch: 29 loss: 0.282851621418\n",
      "epoch: 30 loss: 0.28257651094\n",
      "epoch: 31 loss: 0.282307116783\n",
      "epoch: 32 loss: 0.282043161911\n",
      "epoch: 33 loss: 0.28178438932\n",
      "epoch: 34 loss: 0.281530560015\n",
      "epoch: 35 loss: 0.281281451277\n",
      "epoch: 36 loss: 0.28103685516\n",
      "epoch: 37 loss: 0.280796577182\n",
      "epoch: 38 loss: 0.280560435172\n",
      "epoch: 39 loss: 0.280328258253\n",
      "epoch: 40 loss: 0.280099885935\n",
      "epoch: 41 loss: 0.2798751673\n",
      "epoch: 42 loss: 0.279653960272\n",
      "epoch: 43 loss: 0.279436130958\n",
      "epoch: 44 loss: 0.27922155305\n",
      "epoch: 45 loss: 0.27901010728\n",
      "epoch: 46 loss: 0.278801680923\n",
      "epoch: 47 loss: 0.278596167346\n",
      "epoch: 48 loss: 0.278393465591\n",
      "epoch: 49 loss: 0.278193479995\n",
      "epoch: 50 loss: 0.277996119838\n",
      "epoch: 51 loss: 0.277801299022\n",
      "epoch: 52 loss: 0.277608935772\n",
      "epoch: 53 loss: 0.277418952362\n",
      "epoch: 54 loss: 0.27723127486\n",
      "epoch: 55 loss: 0.277045832895\n",
      "epoch: 56 loss: 0.276862559434\n",
      "epoch: 57 loss: 0.276681390588\n",
      "epoch: 58 loss: 0.276502265416\n",
      "epoch: 59 loss: 0.276325125755\n",
      "epoch: 60 loss: 0.276149916057\n",
      "epoch: 61 loss: 0.275976583239\n",
      "epoch: 62 loss: 0.275805076539\n",
      "epoch: 63 loss: 0.275635347389\n",
      "epoch: 64 loss: 0.275467349288\n",
      "epoch: 65 loss: 0.275301037693\n",
      "epoch: 66 loss: 0.275136369907\n",
      "epoch: 67 loss: 0.274973304982\n",
      "epoch: 68 loss: 0.274811803624\n",
      "epoch: 69 loss: 0.274651828108\n",
      "epoch: 70 loss: 0.274493342193\n",
      "epoch: 71 loss: 0.274336311046\n",
      "epoch: 72 loss: 0.27418070117\n",
      "epoch: 73 loss: 0.274026480335\n",
      "epoch: 74 loss: 0.273873617518\n",
      "epoch: 75 loss: 0.273722082839\n",
      "epoch: 76 loss: 0.273571847506\n",
      "epoch: 77 loss: 0.273422883762\n",
      "epoch: 78 loss: 0.273275164835\n",
      "epoch: 79 loss: 0.273128664894\n",
      "epoch: 80 loss: 0.272983358998\n",
      "epoch: 81 loss: 0.272839223062\n",
      "epoch: 82 loss: 0.272696233809\n",
      "epoch: 83 loss: 0.272554368742\n",
      "epoch: 84 loss: 0.272413606102\n",
      "epoch: 85 loss: 0.272273924837\n",
      "epoch: 86 loss: 0.272135304571\n",
      "epoch: 87 loss: 0.271997725574\n",
      "epoch: 88 loss: 0.271861168732\n",
      "epoch: 89 loss: 0.271725615524\n",
      "epoch: 90 loss: 0.271591047993\n",
      "epoch: 91 loss: 0.271457448726\n",
      "epoch: 92 loss: 0.271324800827\n",
      "epoch: 93 loss: 0.271193087897\n",
      "epoch: 94 loss: 0.271062294017\n",
      "epoch: 95 loss: 0.270932403724\n",
      "epoch: 96 loss: 0.270803401992\n",
      "epoch: 97 loss: 0.270675274221\n",
      "epoch: 98 loss: 0.270548006213\n",
      "epoch: 99 loss: 0.270421584161\n",
      "epoch: 100 loss: 0.27029599463\n",
      "epoch: 101 loss: 0.270171224547\n",
      "epoch: 102 loss: 0.270047261184\n",
      "epoch: 103 loss: 0.269924092147\n",
      "epoch: 104 loss: 0.269801705361\n",
      "epoch: 105 loss: 0.269680089063\n",
      "epoch: 106 loss: 0.269559231784\n",
      "epoch: 107 loss: 0.269439122345\n",
      "epoch: 108 loss: 0.269319749841\n",
      "epoch: 109 loss: 0.269201103637\n",
      "epoch: 110 loss: 0.269083173352\n",
      "epoch: 111 loss: 0.268965948855\n",
      "epoch: 112 loss: 0.268849420257\n",
      "epoch: 113 loss: 0.268733577897\n",
      "epoch: 114 loss: 0.26861841234\n",
      "epoch: 115 loss: 0.268503914369\n",
      "epoch: 116 loss: 0.268390074973\n",
      "epoch: 117 loss: 0.268276885347\n",
      "epoch: 118 loss: 0.268164336878\n",
      "epoch: 119 loss: 0.268052421146\n",
      "epoch: 120 loss: 0.267941129913\n",
      "epoch: 121 loss: 0.267830455119\n",
      "epoch: 122 loss: 0.267720388876\n",
      "epoch: 123 loss: 0.267610923464\n",
      "epoch: 124 loss: 0.267502051323\n",
      "epoch: 125 loss: 0.267393765051\n",
      "epoch: 126 loss: 0.267286057399\n",
      "epoch: 127 loss: 0.267178921264\n",
      "epoch: 128 loss: 0.267072349687\n",
      "epoch: 129 loss: 0.266966335848\n",
      "epoch: 130 loss: 0.266860873062\n",
      "epoch: 131 loss: 0.266755954776\n",
      "epoch: 132 loss: 0.266651574564\n",
      "epoch: 133 loss: 0.266547726123\n",
      "epoch: 134 loss: 0.266444403272\n",
      "epoch: 135 loss: 0.266341599946\n",
      "epoch: 136 loss: 0.266239310195\n",
      "epoch: 137 loss: 0.266137528177\n",
      "epoch: 138 loss: 0.266036248163\n",
      "epoch: 139 loss: 0.265935464523\n",
      "epoch: 140 loss: 0.265835171734\n",
      "epoch: 141 loss: 0.26573536437\n",
      "epoch: 142 loss: 0.265636037103\n",
      "epoch: 143 loss: 0.265537184698\n",
      "epoch: 144 loss: 0.265438802015\n",
      "epoch: 145 loss: 0.265340884\n",
      "epoch: 146 loss: 0.26524342569\n",
      "epoch: 147 loss: 0.265146422205\n",
      "epoch: 148 loss: 0.265049868749\n",
      "epoch: 149 loss: 0.264953760607\n",
      "epoch: 150 loss: 0.264858093144\n",
      "epoch: 151 loss: 0.2647628618\n",
      "epoch: 152 loss: 0.264668062094\n",
      "epoch: 153 loss: 0.264573689616\n",
      "epoch: 154 loss: 0.264479740029\n",
      "epoch: 155 loss: 0.264386209067\n",
      "epoch: 156 loss: 0.26429309253\n",
      "epoch: 157 loss: 0.264200386288\n",
      "epoch: 158 loss: 0.264108086276\n",
      "epoch: 159 loss: 0.264016188493\n",
      "epoch: 160 loss: 0.263924688999\n",
      "epoch: 161 loss: 0.263833583918\n",
      "epoch: 162 loss: 0.263742869433\n",
      "epoch: 163 loss: 0.263652541784\n",
      "epoch: 164 loss: 0.263562597271\n",
      "epoch: 165 loss: 0.263473032249\n",
      "epoch: 166 loss: 0.263383843127\n",
      "epoch: 167 loss: 0.263295026369\n",
      "epoch: 168 loss: 0.263206578492\n",
      "epoch: 169 loss: 0.263118496064\n",
      "epoch: 170 loss: 0.263030775703\n",
      "epoch: 171 loss: 0.262943414077\n",
      "epoch: 172 loss: 0.262856407903\n",
      "epoch: 173 loss: 0.262769753945\n",
      "epoch: 174 loss: 0.262683449013\n",
      "epoch: 175 loss: 0.262597489964\n",
      "epoch: 176 loss: 0.262511873697\n",
      "epoch: 177 loss: 0.262426597159\n",
      "epoch: 178 loss: 0.262341657336\n",
      "epoch: 179 loss: 0.262257051258\n",
      "epoch: 180 loss: 0.262172775996\n",
      "epoch: 181 loss: 0.262088828663\n",
      "epoch: 182 loss: 0.262005206408\n",
      "epoch: 183 loss: 0.261921906423\n",
      "epoch: 184 loss: 0.261838925935\n",
      "epoch: 185 loss: 0.261756262212\n",
      "epoch: 186 loss: 0.261673912556\n",
      "epoch: 187 loss: 0.261591874307\n",
      "epoch: 188 loss: 0.261510144838\n",
      "epoch: 189 loss: 0.261428721559\n",
      "epoch: 190 loss: 0.261347601915\n",
      "epoch: 191 loss: 0.261266783383\n",
      "epoch: 192 loss: 0.261186263472\n",
      "epoch: 193 loss: 0.261106039726\n",
      "epoch: 194 loss: 0.261026109719\n",
      "epoch: 195 loss: 0.260946471057\n",
      "epoch: 196 loss: 0.260867121377\n",
      "epoch: 197 loss: 0.260788058346\n",
      "epoch: 198 loss: 0.26070927966\n",
      "epoch: 199 loss: 0.260630783044\n",
      "epoch: 200 loss: 0.260552566254\n",
      "epoch: 201 loss: 0.260474627071\n",
      "epoch: 202 loss: 0.260396963305\n",
      "epoch: 203 loss: 0.260319572795\n",
      "epoch: 204 loss: 0.260242453404\n",
      "epoch: 205 loss: 0.260165603023\n",
      "epoch: 206 loss: 0.260089019568\n",
      "epoch: 207 loss: 0.26001270098\n",
      "epoch: 208 loss: 0.259936645228\n",
      "epoch: 209 loss: 0.259860850302\n",
      "epoch: 210 loss: 0.259785314218\n",
      "epoch: 211 loss: 0.259710035016\n",
      "epoch: 212 loss: 0.259635010758\n",
      "epoch: 213 loss: 0.259560239531\n",
      "epoch: 214 loss: 0.259485719444\n",
      "epoch: 215 loss: 0.259411448627\n",
      "epoch: 216 loss: 0.259337425233\n",
      "epoch: 217 loss: 0.259263647438\n",
      "epoch: 218 loss: 0.259190113438\n",
      "epoch: 219 loss: 0.259116821449\n",
      "epoch: 220 loss: 0.259043769709\n",
      "epoch: 221 loss: 0.258970956476\n",
      "epoch: 222 loss: 0.258898380029\n",
      "epoch: 223 loss: 0.258826038664\n",
      "epoch: 224 loss: 0.258753930698\n",
      "epoch: 225 loss: 0.258682054469\n",
      "epoch: 226 loss: 0.258610408331\n",
      "epoch: 227 loss: 0.258538990657\n",
      "epoch: 228 loss: 0.25846779984\n",
      "epoch: 229 loss: 0.258396834288\n",
      "epoch: 230 loss: 0.25832609243\n",
      "epoch: 231 loss: 0.258255572711\n",
      "epoch: 232 loss: 0.258185273591\n",
      "epoch: 233 loss: 0.258115193552\n",
      "epoch: 234 loss: 0.258045331087\n",
      "epoch: 235 loss: 0.25797568471\n",
      "epoch: 236 loss: 0.257906252948\n",
      "epoch: 237 loss: 0.257837034347\n",
      "epoch: 238 loss: 0.257768027465\n",
      "epoch: 239 loss: 0.257699230879\n",
      "epoch: 240 loss: 0.25763064318\n",
      "epoch: 241 loss: 0.257562262972\n",
      "epoch: 242 loss: 0.257494088878\n",
      "epoch: 243 loss: 0.257426119533\n",
      "epoch: 244 loss: 0.257358353586\n",
      "epoch: 245 loss: 0.257290789701\n",
      "epoch: 246 loss: 0.257223426557\n",
      "epoch: 247 loss: 0.257156262845\n",
      "epoch: 248 loss: 0.257089297271\n",
      "epoch: 249 loss: 0.257022528554\n",
      "epoch: 250 loss: 0.256955955427\n",
      "epoch: 251 loss: 0.256889576635\n",
      "epoch: 252 loss: 0.256823390935\n",
      "epoch: 253 loss: 0.2567573971\n",
      "epoch: 254 loss: 0.256691593912\n",
      "epoch: 255 loss: 0.256625980168\n",
      "epoch: 256 loss: 0.256560554675\n",
      "epoch: 257 loss: 0.256495316255\n",
      "epoch: 258 loss: 0.256430263738\n",
      "epoch: 259 loss: 0.256365395969\n",
      "epoch: 260 loss: 0.256300711804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 261 loss: 0.256236210109\n",
      "epoch: 262 loss: 0.256171889761\n",
      "epoch: 263 loss: 0.256107749651\n",
      "epoch: 264 loss: 0.256043788679\n",
      "epoch: 265 loss: 0.255980005755\n",
      "epoch: 266 loss: 0.255916399801\n",
      "epoch: 267 loss: 0.255852969749\n",
      "epoch: 268 loss: 0.255789714542\n",
      "epoch: 269 loss: 0.255726633133\n",
      "epoch: 270 loss: 0.255663724485\n",
      "epoch: 271 loss: 0.255600987571\n",
      "epoch: 272 loss: 0.255538421373\n",
      "epoch: 273 loss: 0.255476024884\n",
      "epoch: 274 loss: 0.255413797107\n",
      "epoch: 275 loss: 0.255351737053\n",
      "epoch: 276 loss: 0.255289843743\n",
      "epoch: 277 loss: 0.255228116208\n",
      "epoch: 278 loss: 0.255166553487\n",
      "epoch: 279 loss: 0.25510515463\n",
      "epoch: 280 loss: 0.255043918692\n",
      "epoch: 281 loss: 0.254982844741\n",
      "epoch: 282 loss: 0.254921931852\n",
      "epoch: 283 loss: 0.254861179107\n",
      "epoch: 284 loss: 0.2548005856\n",
      "epoch: 285 loss: 0.25474015043\n",
      "epoch: 286 loss: 0.254679872706\n",
      "epoch: 287 loss: 0.254619751545\n",
      "epoch: 288 loss: 0.254559786072\n",
      "epoch: 289 loss: 0.254499975419\n",
      "epoch: 290 loss: 0.254440318728\n",
      "epoch: 291 loss: 0.254380815147\n",
      "epoch: 292 loss: 0.254321463832\n",
      "epoch: 293 loss: 0.254262263946\n",
      "epoch: 294 loss: 0.254203214662\n",
      "epoch: 295 loss: 0.254144315157\n",
      "epoch: 296 loss: 0.254085564618\n",
      "epoch: 297 loss: 0.254026962237\n",
      "epoch: 298 loss: 0.253968507216\n",
      "epoch: 299 loss: 0.253910198761\n",
      "epoch: 300 loss: 0.253852036088\n",
      "epoch: 301 loss: 0.253794018416\n",
      "epoch: 302 loss: 0.253736144974\n",
      "epoch: 303 loss: 0.253678414998\n",
      "epoch: 304 loss: 0.253620827727\n",
      "epoch: 305 loss: 0.253563382411\n",
      "epoch: 306 loss: 0.253506078304\n",
      "epoch: 307 loss: 0.253448914667\n",
      "epoch: 308 loss: 0.253391890766\n",
      "epoch: 309 loss: 0.253335005876\n",
      "epoch: 310 loss: 0.253278259276\n",
      "epoch: 311 loss: 0.253221650252\n",
      "epoch: 312 loss: 0.253165178096\n",
      "epoch: 313 loss: 0.253108842106\n",
      "epoch: 314 loss: 0.253052641585\n",
      "epoch: 315 loss: 0.252996575843\n",
      "epoch: 316 loss: 0.252940644196\n",
      "epoch: 317 loss: 0.252884845964\n",
      "epoch: 318 loss: 0.252829180475\n",
      "epoch: 319 loss: 0.25277364706\n",
      "epoch: 320 loss: 0.252718245058\n",
      "epoch: 321 loss: 0.252662973812\n",
      "epoch: 322 loss: 0.25260783267\n",
      "epoch: 323 loss: 0.252552820988\n",
      "epoch: 324 loss: 0.252497938123\n",
      "epoch: 325 loss: 0.252443183441\n",
      "epoch: 326 loss: 0.252388556312\n",
      "epoch: 327 loss: 0.25233405611\n",
      "epoch: 328 loss: 0.252279682215\n",
      "epoch: 329 loss: 0.252225434012\n",
      "epoch: 330 loss: 0.252171310891\n",
      "epoch: 331 loss: 0.252117312246\n",
      "epoch: 332 loss: 0.252063437478\n",
      "epoch: 333 loss: 0.25200968599\n",
      "epoch: 334 loss: 0.251956057192\n",
      "epoch: 335 loss: 0.251902550497\n",
      "epoch: 336 loss: 0.251849165324\n",
      "epoch: 337 loss: 0.251795901095\n",
      "epoch: 338 loss: 0.251742757238\n",
      "epoch: 339 loss: 0.251689733186\n",
      "epoch: 340 loss: 0.251636828373\n",
      "epoch: 341 loss: 0.251584042241\n",
      "epoch: 342 loss: 0.251531374236\n",
      "epoch: 343 loss: 0.251478823806\n",
      "epoch: 344 loss: 0.251426390405\n",
      "epoch: 345 loss: 0.251374073491\n",
      "epoch: 346 loss: 0.251321872525\n",
      "epoch: 347 loss: 0.251269786975\n",
      "epoch: 348 loss: 0.25121781631\n",
      "epoch: 349 loss: 0.251165960003\n",
      "epoch: 350 loss: 0.251114217534\n",
      "epoch: 351 loss: 0.251062588385\n",
      "epoch: 352 loss: 0.251011072041\n",
      "epoch: 353 loss: 0.250959667992\n",
      "epoch: 354 loss: 0.250908375732\n",
      "epoch: 355 loss: 0.250857194758\n",
      "epoch: 356 loss: 0.250806124572\n",
      "epoch: 357 loss: 0.250755164678\n",
      "epoch: 358 loss: 0.250704314586\n",
      "epoch: 359 loss: 0.250653573806\n",
      "epoch: 360 loss: 0.250602941855\n",
      "epoch: 361 loss: 0.250552418252\n",
      "epoch: 362 loss: 0.250502002521\n",
      "epoch: 363 loss: 0.250451694187\n",
      "epoch: 364 loss: 0.25040149278\n",
      "epoch: 365 loss: 0.250351397833\n",
      "epoch: 366 loss: 0.250301408883\n",
      "epoch: 367 loss: 0.25025152547\n",
      "epoch: 368 loss: 0.250201747136\n",
      "epoch: 369 loss: 0.25015207343\n",
      "epoch: 370 loss: 0.250102503899\n",
      "epoch: 371 loss: 0.250053038098\n",
      "epoch: 372 loss: 0.250003675581\n",
      "epoch: 373 loss: 0.24995441591\n",
      "epoch: 374 loss: 0.249905258645\n",
      "epoch: 375 loss: 0.249856203353\n",
      "epoch: 376 loss: 0.249807249601\n",
      "epoch: 377 loss: 0.249758396962\n",
      "epoch: 378 loss: 0.24970964501\n",
      "epoch: 379 loss: 0.249660993323\n",
      "epoch: 380 loss: 0.249612441481\n",
      "epoch: 381 loss: 0.249563989067\n",
      "epoch: 382 loss: 0.249515635668\n",
      "epoch: 383 loss: 0.249467380873\n",
      "epoch: 384 loss: 0.249419224273\n",
      "epoch: 385 loss: 0.249371165464\n",
      "epoch: 386 loss: 0.249323204043\n",
      "epoch: 387 loss: 0.249275339611\n",
      "epoch: 388 loss: 0.249227571769\n",
      "epoch: 389 loss: 0.249179900125\n",
      "epoch: 390 loss: 0.249132324286\n",
      "epoch: 391 loss: 0.249084843863\n",
      "epoch: 392 loss: 0.24903745847\n",
      "epoch: 393 loss: 0.248990167723\n",
      "epoch: 394 loss: 0.248942971242\n",
      "epoch: 395 loss: 0.248895868646\n",
      "epoch: 396 loss: 0.248848859561\n",
      "epoch: 397 loss: 0.248801943612\n",
      "epoch: 398 loss: 0.248755120428\n",
      "epoch: 399 loss: 0.248708389641\n",
      "epoch: 400 loss: 0.248661750884\n",
      "epoch: 401 loss: 0.248615203794\n",
      "epoch: 402 loss: 0.248568748008\n",
      "epoch: 403 loss: 0.248522383168\n",
      "epoch: 404 loss: 0.248476108917\n",
      "epoch: 405 loss: 0.248429924901\n",
      "epoch: 406 loss: 0.248383830767\n",
      "epoch: 407 loss: 0.248337826165\n",
      "epoch: 408 loss: 0.248291910748\n",
      "epoch: 409 loss: 0.24824608417\n",
      "epoch: 410 loss: 0.248200346089\n",
      "epoch: 411 loss: 0.248154696162\n",
      "epoch: 412 loss: 0.248109134052\n",
      "epoch: 413 loss: 0.248063659422\n",
      "epoch: 414 loss: 0.248018271937\n",
      "epoch: 415 loss: 0.247972971264\n",
      "epoch: 416 loss: 0.247927757074\n",
      "epoch: 417 loss: 0.247882629038\n",
      "epoch: 418 loss: 0.24783758683\n",
      "epoch: 419 loss: 0.247792630126\n",
      "epoch: 420 loss: 0.247747758604\n",
      "epoch: 421 loss: 0.247702971944\n",
      "epoch: 422 loss: 0.247658269828\n",
      "epoch: 423 loss: 0.247613651939\n",
      "epoch: 424 loss: 0.247569117963\n",
      "epoch: 425 loss: 0.247524667588\n",
      "epoch: 426 loss: 0.247480300505\n",
      "epoch: 427 loss: 0.247436016404\n",
      "epoch: 428 loss: 0.247391814979\n",
      "epoch: 429 loss: 0.247347695925\n",
      "epoch: 430 loss: 0.247303658939\n",
      "epoch: 431 loss: 0.247259703722\n",
      "epoch: 432 loss: 0.247215829972\n",
      "epoch: 433 loss: 0.247172037394\n",
      "epoch: 434 loss: 0.247128325692\n",
      "epoch: 435 loss: 0.247084694571\n",
      "epoch: 436 loss: 0.247041143741\n",
      "epoch: 437 loss: 0.24699767291\n",
      "epoch: 438 loss: 0.24695428179\n",
      "epoch: 439 loss: 0.246910970095\n",
      "epoch: 440 loss: 0.246867737539\n",
      "epoch: 441 loss: 0.246824583839\n",
      "epoch: 442 loss: 0.246781508714\n",
      "epoch: 443 loss: 0.246738511882\n",
      "epoch: 444 loss: 0.246695593067\n",
      "epoch: 445 loss: 0.24665275199\n",
      "epoch: 446 loss: 0.246609988378\n",
      "epoch: 447 loss: 0.246567301956\n",
      "epoch: 448 loss: 0.246524692452\n",
      "epoch: 449 loss: 0.246482159596\n",
      "epoch: 450 loss: 0.24643970312\n",
      "epoch: 451 loss: 0.246397322755\n",
      "epoch: 452 loss: 0.246355018237\n",
      "epoch: 453 loss: 0.246312789301\n",
      "epoch: 454 loss: 0.246270635684\n",
      "epoch: 455 loss: 0.246228557125\n",
      "epoch: 456 loss: 0.246186553366\n",
      "epoch: 457 loss: 0.246144624146\n",
      "epoch: 458 loss: 0.24610276921\n",
      "epoch: 459 loss: 0.246060988303\n",
      "epoch: 460 loss: 0.24601928117\n",
      "epoch: 461 loss: 0.24597764756\n",
      "epoch: 462 loss: 0.24593608722\n",
      "epoch: 463 loss: 0.245894599903\n",
      "epoch: 464 loss: 0.245853185359\n",
      "epoch: 465 loss: 0.245811843341\n",
      "epoch: 466 loss: 0.245770573605\n",
      "epoch: 467 loss: 0.245729375906\n",
      "epoch: 468 loss: 0.245688250002\n",
      "epoch: 469 loss: 0.245647195651\n",
      "epoch: 470 loss: 0.245606212613\n",
      "epoch: 471 loss: 0.24556530065\n",
      "epoch: 472 loss: 0.245524459524\n",
      "epoch: 473 loss: 0.245483688999\n",
      "epoch: 474 loss: 0.24544298884\n",
      "epoch: 475 loss: 0.245402358814\n",
      "epoch: 476 loss: 0.245361798689\n",
      "epoch: 477 loss: 0.245321308233\n",
      "epoch: 478 loss: 0.245280887217\n",
      "epoch: 479 loss: 0.245240535413\n",
      "epoch: 480 loss: 0.245200252593\n",
      "epoch: 481 loss: 0.245160038531\n",
      "epoch: 482 loss: 0.245119893002\n",
      "epoch: 483 loss: 0.245079815783\n",
      "epoch: 484 loss: 0.245039806651\n",
      "epoch: 485 loss: 0.244999865385\n",
      "epoch: 486 loss: 0.244959991765\n",
      "epoch: 487 loss: 0.244920185573\n",
      "epoch: 488 loss: 0.24488044659\n",
      "epoch: 489 loss: 0.2448407746\n",
      "epoch: 490 loss: 0.244801169387\n",
      "epoch: 491 loss: 0.244761630737\n",
      "epoch: 492 loss: 0.244722158437\n",
      "epoch: 493 loss: 0.244682752275\n",
      "epoch: 494 loss: 0.24464341204\n",
      "epoch: 495 loss: 0.244604137522\n",
      "epoch: 496 loss: 0.244564928513\n",
      "epoch: 497 loss: 0.244525784804\n",
      "epoch: 498 loss: 0.244486706189\n",
      "epoch: 499 loss: 0.244447692462\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# случайные веса\n",
    "weights = np.random.random(2**20)\n",
    "\n",
    "# эпохи градиентного спуска\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # считаем градиент\n",
    "    gradient = (\n",
    "        train\n",
    "        .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # обновляем веса\n",
    "    weights -= 0.3 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75695942519187587,\n",
       " 0.62823622497408538,\n",
       " 0.62823622497408538,\n",
       " 0.82968514830006856,\n",
       " 0.82968514830006856]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = test.map(lambda x: evaluate(weights, x)).collect()\n",
    "y_score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68776685182786357"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2 = roc_auc_score(y_true, y_score)\n",
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting result.json\n"
     ]
    }
   ],
   "source": [
    "%%file result.json\n",
    "{\n",
    "    \"q1\": 0.68688612967379636,\n",
    "    \"q2\": 0.68776685182786357\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# останавливаем Spark (и YARN приложение)\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-5-advanced-spark-aabb"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
