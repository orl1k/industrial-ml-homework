{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Будем использовать логи прослушивания музыкальных исполнителей в сервисе Яндекс.Музыка.\n",
    "\n",
    "Файл `events.csv` содержит записи вида `Пользователь,Исполнитель,Число прослушиваний,Число пропусков`:\n",
    "```csv\n",
    "userId,artistId,plays,skips\n",
    "0,335,1,0\n",
    "0,708,1,0\n",
    "0,710,2,1\n",
    "0,815,1,1\n",
    "```\n",
    "\n",
    "Вам необходимо проделать следующее:\n",
    "1. **Оставьте в данных только тех пользователей, для которых сумма plays строго больше 1000. Сколько таких пользователей?**\n",
    "2. **В отфильтрованных на первом шаге данных найдите 5 самых популярных по числу пользователей исполнителей (идентификаторы).**\n",
    "\n",
    "Детали:\n",
    "1. Давайте считать, что список прослушиваний одного пользователя всегда помещается в память.\n",
    "\n",
    "Решение сохраните в файл `result.json`. Пример содержимого файла:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 123,\n",
    "    \"q2\": [\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,artistId,plays,skips\r\n",
      "0,335,1,0\r\n",
      "0,708,1,0\r\n",
      "0,710,2,1\r\n",
      "0,815,1,1\r\n"
     ]
    }
   ],
   "source": [
    "# пример содержимого файла\n",
    "! head -n 5 yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 jovyan supergroup        254 2021-05-08 19:44 /yandex_music/README.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup      3.7 M 2021-05-08 19:44 /yandex_music/artists.jsonl\r\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2021-05-08 19:44 /yandex_music/events.csv\r\n"
     ]
    }
   ],
   "source": [
    "# копируем файлы в HDFS\n",
    "! hadoop fs -copyFromLocal yandex_music /\n",
    "! hadoop fs -ls -h /yandex_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,artistId,plays,skips\r\n",
      "0,335,1,0\r\n",
      "0,708,1,0\r\n",
      "0,710,2,1\r\n",
      "0,815,1,1\r\n",
      "0,880,1,1\r\n",
      "0,1091,2,3\r\n",
      "0,1222,1,1\r\n",
      "0,1571,1,2\r\n",
      "0,1592,2,2\r\n"
     ]
    }
   ],
   "source": [
    "# пример содержимого файла\n",
    "! head -n 10 yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import sys\n",
    "\n",
    "first_skip = True\n",
    "for line in sys.stdin:\n",
    "    if first_skip:\n",
    "        first_skip = False\n",
    "        continue\n",
    "    user_id, artist_id, plays, skips = line.split(',')\n",
    "    print(user_id + \"\\t\" + artist_id + \",\" + plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t335,1\r\n",
      "0\t708,1\r\n",
      "0\t710,2\r\n",
      "0\t815,1\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 5 yandex_music/events.csv | python ./mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "plays_sum = 0\n",
    "event_list = []\n",
    "for line in sys.stdin:  # stream is sorted by key\n",
    "    key, value = line.split(\"\\t\")\n",
    "    _, plays = value.split(\",\")\n",
    "    if prev_key is not None and key != prev_key:\n",
    "        # new key in stream, dump previous\n",
    "        if plays_sum > 1000:\n",
    "            for i in event_list:\n",
    "                print(i)\n",
    "        plays_sum = 0\n",
    "        event_list = []\n",
    "    \n",
    "    event_list.append(line[:-1])\n",
    "    plays_sum += int(plays)\n",
    "    prev_key = key\n",
    "\n",
    "# dump last key\n",
    "if plays_sum > 1000:\n",
    "    for i in event_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 yandex_music/events.csv | python ./mapper.py | python ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/filtered': No such file or directory\n",
      "2021-05-08 19:44:40,992 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar] /tmp/streamjob8191031279392158772.jar tmpDir=null\n",
      "2021-05-08 19:44:41,756 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-05-08 19:44:41,898 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-05-08 19:44:42,080 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1620502994344_0001\n",
      "2021-05-08 19:44:43,134 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2021-05-08 19:44:43,575 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-05-08 19:44:44,063 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1620502994344_0001\n",
      "2021-05-08 19:44:44,063 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-05-08 19:44:44,194 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-05-08 19:44:44,194 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-05-08 19:44:44,398 INFO impl.YarnClientImpl: Submitted application application_1620502994344_0001\n",
      "2021-05-08 19:44:44,439 INFO mapreduce.Job: The url to track the job: http://1dca80b665c1:8088/proxy/application_1620502994344_0001/\n",
      "2021-05-08 19:44:44,441 INFO mapreduce.Job: Running job: job_1620502994344_0001\n",
      "2021-05-08 19:44:51,535 INFO mapreduce.Job: Job job_1620502994344_0001 running in uber mode : false\n",
      "2021-05-08 19:44:51,536 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-05-08 19:45:03,732 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2021-05-08 19:45:04,735 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-05-08 19:45:15,766 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-05-08 19:45:17,775 INFO mapreduce.Job: Job job_1620502994344_0001 completed successfully\n",
      "2021-05-08 19:45:17,848 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=49832121\n",
      "\t\tFILE: Number of bytes written=100734839\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49919977\n",
      "\t\tHDFS: Number of bytes written=32322272\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16182272\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9792512\n",
      "\t\tTotal time spent by all map tasks (ms)=31606\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9563\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=31606\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9563\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16182272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9792512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3412505\n",
      "\t\tMap output records=3412502\n",
      "\t\tMap output bytes=43007111\n",
      "\t\tMap output materialized bytes=49832133\n",
      "\t\tInput split bytes=291\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4999\n",
      "\t\tReduce shuffle bytes=49832133\n",
      "\t\tReduce input records=3412502\n",
      "\t\tReduce output records=2560216\n",
      "\t\tSpilled Records=6825004\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=781\n",
      "\t\tCPU time spent (ms)=14180\n",
      "\t\tPhysical memory (bytes) snapshot=1132118016\n",
      "\t\tVirtual memory (bytes) snapshot=9003425792\n",
      "\t\tTotal committed heap usage (bytes)=914882560\n",
      "\t\tPeak Map Physical memory (bytes)=299732992\n",
      "\t\tPeak Map Virtual memory (bytes)=2137772032\n",
      "\t\tPeak Reduce Physical memory (bytes)=254709760\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2590515200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49919686\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32322272\n",
      "2021-05-08 19:45:17,848 INFO streaming.StreamJob: Output directory: /filtered\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /filtered\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /yandex_music/events.csv \\\n",
    "  -output /filtered \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2021-05-08 19:45 /filtered/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup   32322272 2021-05-08 19:45 /filtered/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3117\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /filtered/part-00000 | cut -f1 | uniq | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper2.py\n",
    "import sys\n",
    "\n",
    "first_skip = True\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    artist_id, plays = value.split(\",\")\n",
    "    print(artist_id + '\\t' + \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer2.py\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "count = 0\n",
    "for line in sys.stdin:  # stream is sorted by key\n",
    "    key, value = line.split(\"\\t\")\n",
    "    \n",
    "    if prev_key is not None and key != prev_key:\n",
    "        # new key in stream, dump previous\n",
    "        print(prev_key + \"\\t\" + str(count))\n",
    "        count = 0\n",
    "    \n",
    "    count += int(value)\n",
    "    prev_key = key\n",
    "\n",
    "# dump last key\n",
    "print(prev_key + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/artist_count': No such file or directory\n",
      "2021-05-08 19:45:26,163 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar] /tmp/streamjob7442406271332210491.jar tmpDir=null\n",
      "2021-05-08 19:45:26,969 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-05-08 19:45:27,111 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-05-08 19:45:27,279 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1620502994344_0002\n",
      "2021-05-08 19:45:28,300 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2021-05-08 19:45:28,311 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:9866\n",
      "2021-05-08 19:45:29,134 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2021-05-08 19:45:29,629 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1620502994344_0002\n",
      "2021-05-08 19:45:29,629 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-05-08 19:45:29,764 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-05-08 19:45:29,765 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-05-08 19:45:29,812 INFO impl.YarnClientImpl: Submitted application application_1620502994344_0002\n",
      "2021-05-08 19:45:29,838 INFO mapreduce.Job: The url to track the job: http://1dca80b665c1:8088/proxy/application_1620502994344_0002/\n",
      "2021-05-08 19:45:29,840 INFO mapreduce.Job: Running job: job_1620502994344_0002\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /artist_count\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /filtered/part-00000 \\\n",
    "  -output /artist_count \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper2.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer2.py\" \\\n",
    "  -file mapper2.py \\\n",
    "  -file reducer2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -cat /artist_count/part-00000 | sort -nk2 -r | head -5 | cut -f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -cat /artist_count/part-00000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file result.json\n",
    "{\n",
    "    \"q1\": 3117,\n",
    "    \"q2\": [\n",
    "        11368,\n",
    "        3629,\n",
    "        259,\n",
    "        44148,\n",
    "        23524\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-3-mapreduce"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
